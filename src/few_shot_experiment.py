"""
Few-Shot Learning Experiment-System f√ºr IT-Support-Tickets

Dieses Programm vergleicht verschiedene KI-Modelle (z.B. Llama3.1, Mistral) bei der Klassifikation von IT-Support-Tickets.
Es unterst√ºtzt verschiedene Few-Shot-Strategien, Prompt-Typen und ist f√ºr wissenschaftliche Auswertung optimiert.

Autor: DHBW Student (4. Semester)
"""

import pandas as pd
import requests
import yaml
import json
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, matthews_corrcoef, balanced_accuracy_score
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
import os
import warnings
warnings.filterwarnings('ignore')

def interactive_menu(config):
    def choose_from_list(options, label):
        while True:
            print(f"\n{label}:")
            print("  1) Alle")
            for idx, opt in enumerate(options, start=2):
                print(f"  {idx}) {opt}")
            inp = input(f"Auswahl (z.B. 2,3, 1 = alle) [Default: 1]: ").strip()
            if inp == "" or inp == "1":
                return options
            try:
                nums = [int(x.strip()) for x in inp.split(",")]
                if any(n < 1 or n > len(options)+1 for n in nums):
                    raise ValueError
                if 1 in nums:
                    return options
                return [options[n-2] for n in nums]
            except Exception:
                print("Ung√ºltige Eingabe. Bitte nur Zahlen wie 2,3 oder 1 f√ºr alle eingeben.")
    # Modelle
    all_models = config.get('models', [])
    models = choose_from_list(all_models, "Modelle")
    # Few-Shot-Counts
    all_few_shot = config.get('few_shot_counts', [])
    few_shot_counts = choose_from_list([str(x) for x in all_few_shot], "Few-Shot-Counts")
    few_shot_counts = [int(x) for x in few_shot_counts]
    # Prompt-Typen
    all_prompt_types = config.get('prompt_types', [])
    prompt_types = choose_from_list(all_prompt_types, "Prompt-Typen")
    # Anzahl Tickets
    total_tickets = config.get('experiment', {}).get('total_tickets', 200)
    while True:
        print(f"\nMaximale Ticketanzahl: {total_tickets}")
        ticket_in = input(f"Wie viele Tickets testen? (1 = alle, Default: 5): ").strip()
        if ticket_in == "" or ticket_in == "1":
            n_tickets = total_tickets
            break
        if ticket_in.isdigit() and int(ticket_in) > 0:
            n_tickets = min(int(ticket_in), total_tickets)
            break
        print("Ung√ºltige Eingabe. Bitte eine Zahl eingeben.")
    print(f"\nEinstellungen √ºbernommen:")
    print(f"  Modelle: {', '.join(models)}")
    print(f"  Few-Shot-Counts: {', '.join(map(str, few_shot_counts))}")
    print(f"  Prompt-Typen: {', '.join(prompt_types)}")
    print(f"  Tickets: {n_tickets}")
    return models, few_shot_counts, prompt_types, n_tickets

class FewShotExperiment:
    """
    Hauptklasse f√ºr das Few-Shot Learning Experiment.
    F√ºhrt die Klassifikation, Auswertung und Visualisierung durch.
    """
    def __init__(self, config_path: str = "config.yaml"):
        # Konfiguration laden (z.B. Modelle, Kategorien, Datenpfade)
        self.config = self._load_config(config_path)
        # Test-ID f√ºr eindeutige Ergebnis-Ordner
        self.test_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        # Zufallszahlengenerator f√ºr Reproduzierbarkeit
        np.random.seed(self.config.get('experiment', {}).get('random_seed', 42))

    def _load_config(self, config_path: str) -> Dict:
        """L√§dt die Konfiguration aus der YAML-Datei."""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"‚ùå Fehler beim Laden der Konfiguration: {e}")
            return {}

    def _load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        L√§dt die Ticketdaten und Few-Shot-Beispiele aus Excel-Dateien.
        Die Pfade werden aus der config.yaml gelesen.
        """
        print("üìÇ Lade Daten...")
        data_dir = self.config.get('paths', {}).get('data_dir', 'data/')
        ticket_file = self.config.get('paths', {}).get('ticket_file', 'tickets_simple_10.xlsx')
        fewshot_file = self.config.get('paths', {}).get('fewshot_file', 'tickets_examples.xlsx')
        # Tickets laden
        tickets_path = os.path.join(data_dir, ticket_file)
        tickets_df = pd.read_excel(tickets_path)
        print(f"‚úÖ {len(tickets_df)} Tickets geladen aus {tickets_path}")
        # Few-Shot-Beispiele laden
        fewshot_path = os.path.join(data_dir, fewshot_file)
        few_shot_df = pd.read_excel(fewshot_path)
        print(f"‚úÖ {len(few_shot_df)} Few-Shot-Beispiele geladen aus {fewshot_path}")
        return tickets_df, few_shot_df

    def _create_prompt(self, ticket_text: str, few_shot_examples: List[Dict], prompt_type: str, categories: List[str]) -> str:
        """
        Erstellt einen Prompt f√ºr das LLM.
        - ticket_text: Der Text des zu klassifizierenden Tickets
        - few_shot_examples: Liste von Beispielen (f√ºr Few-Shot-Learning)
        - prompt_type: "structured" oder "unstructured"
        - categories: Liste der m√∂glichen Kategorien
        """
        if prompt_type == "structured":
            # Wissenschaftlich: Klare Instruktion, Definitionen, Format
            prompt = "Klassifiziere das folgende IT-Support-Ticket in eine der Kategorien.\n\n"
            for cat in categories:
                prompt += f"- {cat}\n"
            prompt += "\n"
            for example in few_shot_examples:
                prompt += f"Beispiel:\nTicket: {example['ticket_text']}\nKategorie: {example['label']}\n\n"
            prompt += f"Ticket: {ticket_text}\nKategorie:"
        else:
            # Unstrukturierter Prompt, aber mit klaren Kategorien
            prompt = f"Was f√ºr ein IT-Problem ist das? M√∂gliche Kategorien: {', '.join(categories)}\n\n"
            for example in few_shot_examples:
                prompt += f"Problem: {example['ticket_text']}\nAntwort: {example['label']}\n\n"
            prompt += f"Problem: {ticket_text}\nAntwort:"
        return prompt

    def _query_llm(self, prompt: str, model: str) -> Optional[str]:
        """
        Sendet den Prompt an das gew√ºnschte Modell √ºber Ollama und gibt die Antwort zur√ºck.
        Fehler werden abgefangen und gemeldet.
        """
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.config.get("llm_params", {}).get("temperature", 0.1),
                "top_p": self.config.get("llm_params", {}).get("top_p", 0.9),
                "num_predict": self.config.get("llm_params", {}).get("max_tokens", 50)
            }
        }
        try:
            response = requests.post(
                f"{self.config['ollama']['base_url']}/api/generate",
                json=payload,
                timeout=self.config['ollama']['timeout']
            )
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            else:
                print(f"‚ùå LLM-Fehler: Status {response.status_code}")
                return None
        except Exception as e:
            print(f"‚ùå LLM-Anfrage-Fehler: {e}")
            return None

    def _extract_category(self, response: str, categories: List[str]) -> str:
        """
        Extrahiert die Kategorie aus der LLM-Antwort.
        Gibt die erste Kategorie zur√ºck, falls keine erkannt wird.
        """
        response_lower = response.lower()
        for category in categories:
            if category.lower() in response_lower:
                return category
        return categories[0] if categories else "Unknown"

    def _get_few_shot_examples(self, examples_df: pd.DataFrame, count: int) -> List[Dict]:
        """
        W√§hlt zuf√§llig Few-Shot-Beispiele aus. Anzahl = count.
        Passt die Spaltennamen an die neue Struktur an (label statt kategorie).
        """
        if count == 0:
            return []
        selected = examples_df.sample(min(count, len(examples_df)), random_state=42)
        # Passe die Spaltennamen an: 'label' statt 'kategorie'
        examples = []
        for _, row in selected.iterrows():
            examples.append({
                'ticket_text': row.get('subject', '') + '\n\n' + str(row.get('body', '')),
                'label': row.get('label', '')
            })
        return examples

    def _extract_ground_truth(self, ticket: Dict) -> str:
        """
        Bestimmt die wahre Kategorie (Ground Truth) aus der Spalte 'label'.
        """
        return ticket.get('label', '')

    def _classify_ticket(self, ticket: Dict, model: str, few_shot_count: int, prompt_type: str, examples_df: pd.DataFrame, categories: List[str]) -> Dict:
        """
        Klassifiziert ein einzelnes Ticket mit dem angegebenen Modell und Few-Shot-Strategie.
        Gibt ein Ergebnis-Dictionary zur√ºck.
        """
        ticket_text = f"{ticket.get('subject', '')}\n\n{ticket.get('body', '')}"
        few_shot_examples = self._get_few_shot_examples(examples_df, few_shot_count)
        prompt = self._create_prompt(ticket_text, few_shot_examples, prompt_type, categories)
        response = self._query_llm(prompt, model)
        if response is None:
            return {
                'ticket_id': ticket.get('subject', 'Unknown'),
                'ground_truth': self._extract_ground_truth(ticket),
                'prediction': categories[0],
                'model': model,
                'few_shot_count': few_shot_count,
                'prompt_type': prompt_type,
                'response': 'ERROR',
                'correct': False
            }
        predicted_category = self._extract_category(response, categories)
        ground_truth = self._extract_ground_truth(ticket)
        return {
            'ticket_id': ticket.get('subject', 'Unknown'),
            'ground_truth': ground_truth,
            'prediction': predicted_category,
            'model': model,
            'few_shot_count': few_shot_count,
            'prompt_type': prompt_type,
            'response': response,
            'correct': predicted_category == ground_truth
        }

    def run_experiment(self, models=None, few_shot_counts=None, prompt_types=None, n_tickets=None) -> pd.DataFrame:
        """
        F√ºhrt das vollst√§ndige Experiment durch (jetzt mit flexiblen Parametern):
        - Modelle, Few-Shot-Counts, Prompt-Typen und Ticketanzahl k√∂nnen eingeschr√§nkt werden.
        - Ergebnisse werden in einem eigenen Ordner gespeichert.
        """
        print("üöÄ Starte Few-Shot Experiment f√ºr mehrere Modelle...")
        tickets_df, examples_df = self._load_data()
        if len(tickets_df) == 0:
            print("‚ùå Keine Tickets gefunden!")
            return pd.DataFrame()
        # √úbernehme ggf. reduzierte Einstellungen
        models = models or self.config.get('models', ['llama3.1:8b', 'mistral:7b'])
        few_shot_counts = few_shot_counts or self.config.get('few_shot_counts', [0, 1, 3, 5])
        prompt_types = prompt_types or self.config.get('prompt_types', ['structured', 'unstructured'])
        n_tickets = n_tickets or self.config.get('experiment', {}).get('total_tickets', 200)
        # Tickets ggf. reduzieren
        tickets_df = tickets_df.sample(min(n_tickets, len(tickets_df)), random_state=42)
        categories = self.config.get('categories', [])
        results = []
        for model in models:
            for few_shot_count in few_shot_counts:
                for prompt_type in prompt_types:
                    print(f"\n--- Modell: {model} | Few-Shot: {few_shot_count} | Prompt: {prompt_type} ---")
                    for _, ticket in tickets_df.iterrows():
                        result = self._classify_ticket(ticket, model, few_shot_count, prompt_type, examples_df, categories)
                        results.append(result)
        results_df = pd.DataFrame(results)
        # Ergebnisse speichern
        results_dir = os.path.join(self.config.get('paths', {}).get('results_dir', 'results/'), f"test_{self.test_id}")
        os.makedirs(results_dir, exist_ok=True)
        results_df.to_csv(os.path.join(results_dir, f"results_{self.test_id}.csv"), index=False)
        print(f"\nüíæ Ergebnisse gespeichert unter: {results_dir}")
        return results_df

    def analyze_results(self, results_df: pd.DataFrame) -> Dict:
        """
        Analysiert die Ergebnisse wissenschaftlich:
        - Berechnet verschiedene Metriken (Accuracy, F1, Precision, Recall, MCC, Balanced Accuracy)
        - F√ºhrt ANOVA durch
        - Speichert Analyse als JSON
        """
        print("üìä Analysiere Ergebnisse...")
        analysis = {}
        # 1. Gesamt-Accuracy
        analysis['overall_accuracy'] = results_df['correct'].mean()
        # 2. Metriken pro Bedingung
        condition_metrics = {}
        for condition in results_df.groupby(['model', 'few_shot_count', 'prompt_type']):
            condition_name = f"{condition[0][0]}_{condition[0][1]}shots_{condition[0][2]}"
            condition_df = condition[1]
            y_true = condition_df['ground_truth']
            y_pred = condition_df['prediction']
            accuracy = (y_true == y_pred).mean()
            f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)
            precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
            mcc = matthews_corrcoef(y_true, y_pred) if len(set(y_true)) > 1 else 0.0
            bal_acc = balanced_accuracy_score(y_true, y_pred)
            condition_metrics[condition_name] = {
                'accuracy': accuracy,
                'f1_score': f1,
                'precision': precision,
                'recall': recall,
                'mcc': mcc,
                'balanced_accuracy': bal_acc,
                'n_samples': len(condition_df)
            }
        analysis['condition_metrics'] = condition_metrics
        # 3. ANOVA-Test
        accuracy_data = []
        for condition_name, metrics in condition_metrics.items():
            condition_df = results_df[
                (results_df['model'] == condition_name.split('_')[0]) &
                (results_df['few_shot_count'] == int(condition_name.split('_')[1].replace('shots', ''))) &
                (results_df['prompt_type'] == condition_name.split('_')[2])
            ]
            accuracy_data.append(condition_df['correct'].values)
        if len(accuracy_data) > 1:
            f_stat, p_value = stats.f_oneway(*accuracy_data)
            analysis['anova'] = {
                'f_statistic': f_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        # 4. Speichere Analyse
        results_dir = os.path.join(self.config.get('paths', {}).get('results_dir', 'results/'), f"test_{self.test_id}")
        with open(os.path.join(results_dir, f"analysis_{self.test_id}.json"), "w") as f:
            json.dump(analysis, f, indent=2, default=str)
        print("‚úÖ Analyse abgeschlossen!")
        return analysis

    def create_visualizations(self, results_df: pd.DataFrame, analysis: Dict):
        """
        Erstellt wissenschaftliche Visualisierungen:
        - Speichert alle Plots im Ergebnis-Ordner
        - Verbesserte Achsen, Titel, Farben
        """
        print("üìà Erstelle Visualisierungen...")
        results_dir = os.path.join(self.config.get('paths', {}).get('results_dir', 'results/'), f"test_{self.test_id}")
        # 1. Accuracy-Vergleich
        plt.figure(figsize=(15, 8))
        conditions = list(analysis['condition_metrics'].keys())
        accuracies = [analysis['condition_metrics'][c]['accuracy'] for c in conditions]
        colors = ['skyblue' if 'structured' in c else 'lightcoral' for c in conditions]
        bars = plt.bar(range(len(conditions)), accuracies, color=colors)
        plt.title('Accuracy pro Experiment-Bedingung', fontweight='bold', fontsize=14)
        plt.xlabel('Bedingung (Modell, Few-Shot, Prompt)')
        plt.ylabel('Accuracy')
        plt.xticks(range(len(conditions)), conditions, rotation=45, ha='right')
        plt.ylim(0, 1)
        for i, v in enumerate(accuracies):
            plt.text(i, v + 0.01, f'{v:.2%}', ha='center', va='bottom')
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='skyblue', label='Structured'), Patch(facecolor='lightcoral', label='Unstructured')]
        plt.legend(handles=legend_elements)
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, f"accuracy_{self.test_id}.png"), dpi=300, bbox_inches='tight')
        plt.close()
        # 2. F1-Score Heatmap
        pivot_data = []
        for condition, metrics in analysis['condition_metrics'].items():
            parts = condition.split('_')
            model = parts[0]
            few_shot = int(parts[1].replace('shots', ''))
            prompt_type = parts[2]
            pivot_data.append({
                'model': model,
                'few_shot_count': few_shot,
                'prompt_type': prompt_type,
                'f1_score': metrics['f1_score']
            })
        pivot_df = pd.DataFrame(pivot_data)
        heatmap_data = pivot_df.pivot_table(values='f1_score', index='model', columns='few_shot_count', aggfunc='mean')
        plt.figure(figsize=(10, 6))
        sns.heatmap(heatmap_data, annot=True, fmt='.2%', cmap='Blues', cbar_kws={'label': 'F1-Score'})
        plt.title('F1-Score: Modell vs. Few-Shot-Count', fontweight='bold')
        plt.xlabel('Few-Shot-Count')
        plt.ylabel('Modell')
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, f"f1_heatmap_{self.test_id}.png"), dpi=300, bbox_inches='tight')
        plt.close()
        print("‚úÖ Visualisierungen erstellt!")

    def generate_report(self, results_df: pd.DataFrame, analysis: Dict):
        """
        Erstellt einen wissenschaftlichen Bericht (Markdown) mit allen Ergebnissen und Erkl√§rungen.
        """
        print("üìù Generiere wissenschaftlichen Bericht...")
        results_dir = os.path.join(self.config.get('paths', {}).get('results_dir', 'results/'), f"test_{self.test_id}")
        report = f"""
# Few-Shot Learning Experiment Bericht

**Test-ID:** {self.test_id}  
**Datum:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**Gesamt-Accuracy:** {analysis['overall_accuracy']:.2%}  
**Anzahl Experimente:** {len(results_df)}

## Experiment-Design

- **Modelle:** {', '.join(self.config.get('models', []))}
- **Few-Shot-Counts:** {self.config.get('few_shot_counts', [])}
- **Prompt-Types:** {self.config.get('prompt_types', [])}
- **Kategorien:** {', '.join(self.config.get('categories', []))}

## Ergebnisse pro Bedingung
"""
        for condition, metrics in analysis['condition_metrics'].items():
            report += f"""
### {condition}
- **Accuracy:** {metrics['accuracy']:.2%}
- **F1-Score:** {metrics['f1_score']:.2%}
- **Precision:** {metrics['precision']:.2%}
- **Recall:** {metrics['recall']:.2%}
- **MCC:** {metrics['mcc']:.2%}
- **Balanced Accuracy:** {metrics['balanced_accuracy']:.2%}
- **Stichprobengr√∂√üe:** {metrics['n_samples']}
"""
        if 'anova' in analysis:
            report += f"""
## Statistische Analyse

**ANOVA-Ergebnisse:**
- **F-Statistik:** {analysis['anova']['f_statistic']:.4f}
- **p-Wert:** {analysis['anova']['p_value']:.4f}
- **Statistisch signifikant:** {analysis['anova']['significant']}
"""
        report += f"""
## Schlussfolgerungen

1. **Beste Bedingung:** {max(analysis['condition_metrics'].items(), key=lambda x: x[1]['accuracy'])[0]} mit {max(analysis['condition_metrics'].items(), key=lambda x: x[1]['accuracy'])[1]['accuracy']:.2%} Accuracy
2. **Few-Shot-Learning Effekt:** {'Ja' if any('1shots' in k or '3shots' in k or '5shots' in k for k in analysis['condition_metrics'].keys()) else 'Nein'}
3. **Modell-Unterschiede:** {'Signifikant' if analysis.get('anova', {}).get('significant', False) else 'Nicht signifikant'}

## Wissenschaftliche Hinweise
- **Accuracy**: Anteil korrekt klassifizierter Tickets
- **F1-Score**: Harm. Mittel von Precision und Recall (gut bei unbalancierten Daten)
- **Precision**: Wie viele Vorhersagen waren korrekt?
- **Recall**: Wie viele der echten F√§lle wurden gefunden?
- **MCC**: Matthews Correlation Coefficient (robust bei unbalancierten Klassen)
- **Balanced Accuracy**: Durchschnittliche Recall √ºber alle Klassen

## Reproduzierbarkeit
- Random Seed: {self.config.get('experiment', {}).get('random_seed', 42)}
- Alle Parameter in config.yaml dokumentiert
- Rohdaten, Analyse und Grafiken im Ergebnis-Ordner gespeichert
"""
        with open(os.path.join(results_dir, f"report_{self.test_id}.md"), "w", encoding="utf-8") as f:
            f.write(report)
        print("‚úÖ Bericht generiert!")

def main():
    """
    Hauptfunktion: F√ºhrt das Experiment aus, analysiert und visualisiert die Ergebnisse.
    """
    print("üî¨ Few-Shot Learning Experiment (Llama3.1 vs. Mistral)")
    print("=" * 50)
    experiment = FewShotExperiment()
    # Interaktives Men√º
    models, few_shot_counts, prompt_types, n_tickets = interactive_menu(experiment.config)
    results_df = experiment.run_experiment(models=models, few_shot_counts=few_shot_counts, prompt_types=prompt_types, n_tickets=n_tickets)
    if len(results_df) > 0:
        analysis = experiment.analyze_results(results_df)
        experiment.create_visualizations(results_df, analysis)
        experiment.generate_report(results_df, analysis)
        print("\nüìã Experiment-Zusammenfassung:")
        print(f"   Gesamt-Accuracy: {analysis['overall_accuracy']:.2%}")
        print(f"   Anzahl Experimente: {len(results_df)}")
        print(f"   Test-ID: {experiment.test_id}")
        if 'anova' in analysis:
            print(f"   ANOVA p-Wert: {analysis['anova']['p_value']:.4f}")
            print(f"   Statistisch signifikant: {analysis['anova']['significant']}")
        best_condition = max(analysis['condition_metrics'].items(), key=lambda x: x[1]['accuracy'])
        print(f"   Beste Bedingung: {best_condition[0]} ({best_condition[1]['accuracy']:.2%} Accuracy)")
    print("‚úÖ Experiment abgeschlossen!")

if __name__ == "__main__":
    main() 