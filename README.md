# Wissenschaftliches Few-Shot Learning Experiment

## ğŸ¯ ProjektÃ¼bersicht

Dieses Projekt implementiert ein **wissenschaftlich rigoroses Few-Shot Learning Experiment** fÃ¼r die DHBW Projektarbeit. Es vergleicht verschiedene Large Language Models (LLMs) bei der Klassifikation von IT-Support-Tickets unter verschiedenen experimentellen Bedingungen.

### Wissenschaftliche Standards
- âœ… **Faktorielles 2Ã—4Ã—2 Design** (LLM Ã— Few-Shot-Count Ã— Prompt-Type)
- âœ… **Statistische Power** mit nâ‰¥25 pro Bedingung
- âœ… **VollstÃ¤ndige Few-Shot Progression** (0, 1, 3, 5 Beispiele)
- âœ… **Baseline-Vergleich** mit Zero-shot als Kontrolle
- âœ… **Reproduzierbarkeit** durch Seeding und vollstÃ¤ndige Dokumentation

## ğŸ“Š Experimentelles Design

### Faktoren
- **LLM-Modelle**: Llama3.1:8b, Mistral:7b
- **Few-Shot Bedingungen**: 0-shot (Baseline), 1-shot, 3-shot, 5-shot
- **Prompt-Typen**: Strukturiert, Unstrukturiert

### AbhÃ¤ngige Variablen
- Klassifikationsaccuracy
- F1-Score (gewichtet und makro)
- Precision & Recall
- Matthews Correlation Coefficient (MCC)
- Balanced Accuracy

### Kategorien
- **Hardware**: GerÃ¤teprobleme (Laptop, Drucker, Monitor)
- **Software**: Programmprobleme (Excel, VPN, Email)
- **Network**: Netzwerkprobleme (Internet, WLAN, Server)
- **Security**: Sicherheitsprobleme (Phishing, Passwort, Malware)

## ğŸ”§ Installation und Setup

### Voraussetzungen
```bash
# Python 3.8+
# Ollama Server (fÃ¼r LLM-Inferenz)
```

### Installation
```bash
# Repository klonen
git clone <repository-url>
cd few-shot-classification-experiment

# Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # macOS/Linux
# oder
venv\Scripts\activate     # Windows

# Dependencies installieren
pip install -r requirements.txt
```

### Ollama Setup
```bash
# Ollama installieren (https://ollama.ai/)
# Modelle herunterladen
ollama pull llama3.1:8b
ollama pull mistral:7b

# Ollama Server starten
ollama serve
```

## ğŸš€ Experiment ausfÃ¼hren

### Schnellstart
```bash
python src/few_shot_experiment.py
```

### Wissenschaftlicher Workflow
Das Experiment durchlÃ¤uft folgende Phasen:

1. **ğŸ“‹ Experimentelle Validierung**
   - ÃœberprÃ¼fung der Konfiguration
   - Validierung des Experimental Design
   - Power-Analyse

2. **ğŸ“Š Power-Analyse**
   - Berechnung erforderlicher StichprobengrÃ¶ÃŸen
   - SchÃ¤tzung der Experimentdauer
   - Validierung der statistischen Power

3. **ğŸ¯ Experimentelle Parameter**
   - Anzeige aller Faktoren und Level
   - BestÃ¤tigung durch Benutzer
   - Final setup validation

4. **ğŸš€ Experiment-AusfÃ¼hrung**
   - Fortschrittsverfolgung mit Progress Bars
   - Fehlerbehandlung und Retry-Mechanismen
   - Zwischenspeicherung alle 50 Klassifikationen

5. **ğŸ“Š Wissenschaftliche Analyse**
   - Deskriptive Statistiken mit Konfidenzintervallen
   - Mehrfaktorielle ANOVA
   - Post-hoc Tests (Tukey HSD)
   - EffektgrÃ¶ÃŸenberechnung (Cohen's f)
   - Assumption Testing

6. **ğŸ“ˆ Visualisierungen**
   - Few-Shot Progression Plots
   - Model Comparison Charts
   - Statistical Heatmaps
   - Effect Size Forest Plots
   - Confusion Matrices

7. **ğŸ“ Wissenschaftlicher Bericht**
   - Markdown Report mit allen Ergebnissen
   - LaTeX-ready Tables
   - Reproduzierbarkeits-Informationen

## ğŸ“ Projektstruktur

```
few-shot-classification-experiment/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ few_shot_experiment.py    # Hauptexperimentcode
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ tickets_extended_187.xlsx # Hauptdatensatz (186 Tickets)
â”‚   â””â”€â”€ tickets_examples.xlsx     # Few-Shot Beispiele
â”œâ”€â”€ results/                      # Experimentelle Ergebnisse
â”‚   â””â”€â”€ test_YYYYMMDD_HHMMSS/    # Eindeutige Test-IDs
â”‚       â”œâ”€â”€ results_*.csv         # Rohdaten
â”‚       â”œâ”€â”€ comprehensive_analysis_*.json # Statistische Analyse
â”‚       â”œâ”€â”€ report_*.md           # Wissenschaftlicher Bericht
â”‚       â”œâ”€â”€ *.png                 # Visualisierungen
â”‚       â””â”€â”€ experiment_*.log      # VollstÃ¤ndiges Log
â”œâ”€â”€ config.yaml                  # Experimentelle Konfiguration
â”œâ”€â”€ requirements.txt             # Python Dependencies
â””â”€â”€ README.md                   # Diese Datei
```

## âš™ï¸ Konfiguration

Die Datei `config.yaml` enthÃ¤lt alle experimentellen Parameter:

```yaml
experiment:
  random_seed: 42                    # Reproduzierbarkeit
  total_experimental_tickets: 200    # Gesamtanzahl Tickets
  min_per_condition: 25             # Minimum pro Bedingung

quality_control:
  min_ticket_length: 20             # MindestlÃ¤nge pro Ticket
  max_retry_attempts: 3             # LLM Retry-Versuche
  timeout_seconds: 45               # Timeout pro Request

statistics:
  alpha_level: 0.05                 # Signifikanzniveau
  power_target: 0.80               # Statistische Power
  bonferroni_correction: true      # Multiple Comparisons
```

## ğŸ“Š Beispiel-Ergebnisse

### Statistische Ausgabe
```
ğŸ¯ EXPERIMENT ZUSAMMENFASSUNG
Overall Accuracy: 0.847
F1-Score (Weighted): 0.845
Matthews Correlation: 0.798
Balanced Accuracy: 0.849

ğŸ“Š ANOVA Ergebnisse:
   model: p = 0.0123 *
   few_shot_count: p = 0.0001 ***
   prompt_type: p = 0.2341 ns

ğŸ“ EffektgrÃ¶ÃŸen (Cohen's f):
   model: 0.142 (small)
   few_shot_count: 0.287 (medium)
   prompt_type: 0.089 (negligible)
```

### Generierte Dateien
- **Rohdaten**: `results_TIMESTAMP.csv`
- **Statistische Analyse**: `comprehensive_analysis_TIMESTAMP.json`
- **Visualisierungen**: Multiple PNG-Dateien
- **Wissenschaftlicher Bericht**: `report_TIMESTAMP.md`
- **Experiment-Log**: `experiment_TIMESTAMP.log`

## ğŸ”¬ Wissenschaftliche ValiditÃ¤t

### Statistische RigorositÃ¤t
- **Faktorielles Design**: VollstÃ¤ndige 2Ã—4Ã—2 Faktorenstruktur
- **Power-Analyse**: Minimum n=25 pro Bedingung fÃ¼r 80% Power
- **Multiple Comparisons**: Bonferroni-Korrektur
- **Effect Sizes**: Cohen's f fÃ¼r praktische Signifikanz
- **Assumption Testing**: NormalitÃ¤t und HomoskedastizitÃ¤t

### Reproduzierbarkeit
- **Seeding**: Alle Zufallsprozesse deterministisch
- **Logging**: VollstÃ¤ndige Dokumentation aller Schritte
- **Versionierung**: Alle Parameter in config.yaml
- **Metadata**: Experimentelle Bedingungen gespeichert

### QualitÃ¤tskontrolle
- **Data Validation**: Automatische DatenqualitÃ¤tsprÃ¼fung
- **Error Handling**: Robuste Fehlerbehandlung
- **Progress Monitoring**: Real-time Fortschrittsverfolgung
- **Intermediate Saves**: Datensicherheit durch Zwischenspeicherung

## ğŸ“š FÃ¼r DHBW Projektarbeit

### Verwendung in der Arbeit
1. **Methodenkapitel**: Experimentelles Design aus config.yaml
2. **Ergebniskapitel**: Analyse aus JSON und Markdown Report
3. **Diskussion**: EffektgrÃ¶ÃŸen und praktische Signifikanz
4. **Anhang**: VollstÃ¤ndige Rohdaten und Reproduzierbarkeit

### LaTeX Integration
```latex
% Beispiel-Tabelle (wird automatisch generiert)
\input{tables/anova_results.tex}
\input{tables/descriptive_statistics.tex}
```

## ğŸ”§ Erweiterte Nutzung

### Custom Analysis
```python
from src.few_shot_experiment import FewShotExperiment

# Eigenes Experiment
experiment = FewShotExperiment('custom_config.yaml')
results = experiment.run_experiment()
analysis = experiment.analyze_results(results)
```

### Parameter Tuning
```yaml
# config.yaml anpassen fÃ¼r verschiedene Experimente
models:
  - "gpt-4"          # Wenn verfÃ¼gbar
  - "claude-3"       # Wenn verfÃ¼gbar
  
few_shot_counts: [0, 2, 4, 6, 8]  # Andere Progression
```

## ğŸ¤ Beitragen

### Issues
- Experimentelle Verbesserungen
- Statistische Methoden
- Visualisierungsoptionen

### Pull Requests
1. Fork des Repositories
2. Feature Branch erstellen
3. Tests durchfÃ¼hren
4. Pull Request einreichen

## ğŸ“„ Lizenz

Dieses Projekt ist fÃ¼r akademische Zwecke der DHBW entwickelt.

## ğŸ“ Support

Bei Fragen zum wissenschaftlichen Design oder der Implementierung:
- GitHub Issues fÃ¼r technische Probleme
- DHBW Betreuer fÃ¼r akademische Fragen

---

**Hinweis**: Dieses Experiment wurde nach hÃ¶chsten wissenschaftlichen Standards entwickelt und ist fÃ¼r akademische Publikationen geeignet. 